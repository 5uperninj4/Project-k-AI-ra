{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pyttsx3 as tts\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the kitchen stinks . ', \" i'll throw out the garbage . \"), (\" i'll throw out the garbage . \", ''), ('so dick , how about getting some coffee for tonight ? ', \" coffee ? i don't honestly like that kind of stuff . \"), (\" coffee ? i don't honestly like that kind of stuff . \", ' come on , you can at least try a little , besides your cigarette . '), (' come on , you can at least try a little , besides your cigarette . ', \" what's wrong with that ? cigarette is the thing i go crazy for . \"), (\" what's wrong with that ? cigarette is the thing i go crazy for . \", ' not for me , dick . '), (' not for me , dick . ', ''), ('are things still going badly with your houseguest ? ', \" getting worse . now he's eating me out of house and home . i've tried talking to him but it all goes in one ear and out the other . he makes himself at home , which is fine . but what really gets me is that yesterday he walked into the living room in the raw and i had company over ! that was the last straw . \"), (\" getting worse . now he's eating me out of house and home . i've tried talking to him but it all goes in one ear and out the other . he makes himself at home , which is fine . but what really gets me is that yesterday he walked into the living room in the raw and i had company over ! that was the last straw . \", \" leo , i really think you're beating around the bush with this guy . i know he used to be your best friend in college , but i really think it's time to lay down the law . \"), (\" leo , i really think you're beating around the bush with this guy . i know he used to be your best friend in college , but i really think it's time to lay down the law . \", \" you're right . everything is probably going to come to a head tonight . i'll keep you informed . \")]\n"
     ]
    }
   ],
   "source": [
    "with open(\"C:/Users/logan/Documents/Coding/Python/kAIra/3.0/Main/dialogues.txt\", \"r\", errors=\"ignore\") as file:\n",
    "    data = file.read()\n",
    "    \n",
    "def parse(text):\n",
    "    #text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(r\" â€™ \", \"'\", text)\n",
    "    text = re.sub(r'[A-Z]', lambda match: match.group().lower(), text)\n",
    "    return text\n",
    "\n",
    "# Function to process the contents of the text file\n",
    "def process_text_file(file_path):\n",
    "    # List to store the tuples\n",
    "    tuples_list = []\n",
    "\n",
    "    # Open and read the file\n",
    "    with open(file_path, 'r', errors='ignore') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        # Process each line\n",
    "        for line in lines:\n",
    "            # Strip any leading/trailing whitespace\n",
    "            line = line.strip()\n",
    "\n",
    "            # Split the line by __eou__\n",
    "            entries = line.split('__eou__')\n",
    "\n",
    "            # Create tuples from consecutive entries\n",
    "            for i in range(len(entries) - 1):\n",
    "                tuple_entry = (entries[i], entries[i + 1])\n",
    "                tuples_list.append(tuple_entry)\n",
    "\n",
    "    return tuples_list\n",
    "\n",
    "file_path = 'C:/Users/logan/Documents/Coding/Python/kAIra/3.0/Main/dialogues.txt'\n",
    "result = process_text_file(file_path)\n",
    "print(result[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dummy data for demonstration purposes\n",
    "data = result\n",
    "\n",
    "# Tokenization and vocabulary building\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "for sentence, response in data:\n",
    "    for word in sentence.split() + response.split():\n",
    "        if word not in word2idx:\n",
    "            idx2word[len(word2idx)] = word\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return [word2idx[word] for word in sentence.split()]\n",
    "\n",
    "def detokenize(tokens):\n",
    "    return ' '.join([idx2word[token] for token in tokens])\n",
    "\n",
    "# Preparing input-output pairs\n",
    "input_data = [tokenize(sentence) for sentence, _ in data]\n",
    "target_data = [tokenize(response) for _, response in data]\n",
    "\n",
    "# Padding sequences to ensure uniform length\n",
    "def pad_sequence(seq, max_length):\n",
    "    return seq + [0] * (max_length - len(seq))\n",
    "\n",
    "max_length = max(max(len(seq) for seq in input_data), max(len(seq) for seq in target_data))\n",
    "input_data = [pad_sequence(seq, max_length) for seq in input_data]\n",
    "target_data = [pad_sequence(seq, max_length) for seq in target_data]\n",
    "\n",
    "input_data = torch.tensor(input_data, dtype=torch.long)\n",
    "target_data = torch.tensor(target_data, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "embedding_dim = 10\n",
    "hidden_dim = 50\n",
    "output_dim = vocab_size\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "model = ChatbotModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 10.0787\n",
      "Epoch [2/10], Loss: 9.7455\n",
      "Epoch [3/10], Loss: 9.7611\n",
      "Epoch [4/10], Loss: 9.5394\n",
      "Epoch [5/10], Loss: 9.4493\n",
      "Epoch [6/10], Loss: 9.4212\n",
      "Epoch [7/10], Loss: 9.3673\n",
      "Epoch [8/10], Loss: 9.3838\n",
      "Epoch [9/10], Loss: 9.2180\n",
      "Epoch [10/10], Loss: 9.0523\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "data = torch.randint(0, vocab_size, (1000, 20))\n",
    "targets = torch.randint(0, vocab_size, (1000, 20))\n",
    "\n",
    "dataset = Mydataset(data, targets)\n",
    "data_loader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    for input_data, target_data in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_data)\n",
    "        loss = criterion(output.view(-1, vocab_size), target_data.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    if loss.item() < 0.1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: wakening cleansers trade.why trade.why trade.why trade.why trade.why trade.why trade.why trade.why trade.why trade.why trade.why trade.why trade.why trade.why send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send send\n"
     ]
    }
   ],
   "source": [
    "def predict(sentence):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_seq = torch.tensor([pad_sequence(tokenize(sentence), max_length)], dtype=torch.long)\n",
    "        output = model(input_seq)\n",
    "        output_seq = torch.argmax(output, dim=2).numpy().flatten()\n",
    "        response = detokenize(output_seq)\n",
    "    return response\n",
    "\n",
    "def speak(speech):\n",
    "    tts.speak(speech)\n",
    "\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"quit\":\n",
    "        break\n",
    "    response = predict(parse(user_input))\n",
    "    print(f\"Bot: {response}\")\n",
    "    speak(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
