{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K(AI)ra version 3.1.2\n",
    "\n",
    "\n",
    "### Changelog\n",
    "-   reworked entire structure\n",
    "-   added printout clarity\n",
    "-   changed dataset from cornell movie dialogues corpus to daily dialogues\n",
    "\n",
    "### Fixes / Improvements / additions needed\n",
    "-   pyVTS integration\n",
    "-   speech synthesis\n",
    "-   output length\n",
    "-   output logs\n",
    "-   control panel\n",
    "-   speech recognition\n",
    "-   screen vision\n",
    "-   username = voice detection\n",
    "-   new word adding to dictionary\n",
    "-   latency\n",
    "-   dataset modernisation\n",
    "-   automatic data collection\n",
    "-   model saving/loading\n",
    "-   multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\logan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pyttsx3 as tts\n",
    "import re\n",
    "import threading\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#       VARIABLE              DEFAULT             DESCRIPTION\n",
    "#\n",
    "#       batch_size            (32)                Lower batch size causes faster training, but lower accuracy\n",
    "#       embedding_dim         (100)               Higher embedding dimensions can cause higher performance, but too high and the performance is decreased.\n",
    "#       hidden_dim            (128)               Deeper models are better at generalization, but wider models are better at memorization.\n",
    "#       learning_rate         (0.001)             Higher learning rate causes faster changes.\n",
    "#\n",
    "#       stoptype              (t, e, l)           When to stop training t = time, e = epochs, l = loss. training will stop at epochs if it has not reached loss or time.\n",
    "#\n",
    "#       acceptable_loss       (0.001)             An amount considered to be acceptable. training will stop here even if the amount of epochs is not reached.\n",
    "#       stop_minutes          (480)               Amount of minutes until training stops\n",
    "#       epochs                (10000)             How many generations to train for. The more, the longer training but the more accurate the model is.\n",
    "\n",
    "\n",
    "batch_size       = 32           \n",
    "embedding_dim    = 64           \n",
    "hidden_dim       = 128           \n",
    "learning_rate    = 0.001        \n",
    "\n",
    "stoptype         = \"l\"\n",
    "\n",
    "acceptable_loss  = 0.001        \n",
    "stop_minutes     = 10    \n",
    "epochs           = 100\n",
    "\n",
    "file_path        = 'C:/Users/logan/Documents/Coding/Python/kAIra/3.0/Main/dialogues_text.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The kitchen stinks . ', \" I'll throw out the garbage . \"), (\" I'll throw out the garbage . \", ''), ('So Dick , how about getting some coffee for tonight ? ', ' Coffee ? I don â€™ t honestly like that kind of stuff . '), (' Coffee ? I don â€™ t honestly like that kind of stuff . ', ' Come on , you can at least try a little , besides your cigarette . '), (' Come on , you can at least try a little , besides your cigarette . ', ' What â€™ s wrong with that ? Cigarette is the thing I go crazy for . '), (' What â€™ s wrong with that ? Cigarette is the thing I go crazy for . ', ' Not for me , Dick . '), (' Not for me , Dick . ', ''), ('Are things still going badly with your houseguest ? ', ' Getting worse . Now he â€™ s eating me out of house and home . I â€™ Ve tried talking to him but it all goes in one ear and out the other . He makes himself at home , which is fine . But what really gets me is that yesterday he walked into the living room in the raw and I had company over ! That was the last straw . '), (' Getting worse . Now he â€™ s eating me out of house and home . I â€™ Ve tried talking to him but it all goes in one ear and out the other . He makes himself at home , which is fine . But what really gets me is that yesterday he walked into the living room in the raw and I had company over ! That was the last straw . ', ' Leo , I really think you â€™ re beating around the bush with this guy . I know he used to be your best friend in college , but I really think it â€™ s time to lay down the law . '), (' Leo , I really think you â€™ re beating around the bush with this guy . I know he used to be your best friend in college , but I really think it â€™ s time to lay down the law . ', ' You â€™ re right . Everything is probably going to come to a head tonight . I â€™ ll keep you informed . ')]\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, \"r\", errors=\"ignore\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "def parse(text):\n",
    "    text = re.sub(r\" â€™ \", \"'\", text)\n",
    "    text = re.sub(r'[A-Z]', lambda match: match.group().lower(), text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_text_file(file_path):\n",
    "    tuples_list = []\n",
    "    with open(file_path, 'r', errors='ignore') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            entries = line.split('__eou__')\n",
    "            for i in range(len(entries) - 1):\n",
    "                tuple_entry = (entries[i], entries[i + 1])\n",
    "                tuples_list.append(tuple_entry)\n",
    "    \n",
    "    return tuples_list\n",
    "\n",
    "\n",
    "data = process_text_file(file_path)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and vocabulary building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278\n",
      "[[1, 2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[5, 6, 7, 8, 9, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "word2idx = {}\n",
    "idx2word = {}\n",
    "\n",
    "idx2word[0] = '<EOS>'\n",
    "word2idx['<EOS>'] = 0\n",
    "\n",
    "for sentence, response in data:\n",
    "    for word in sentence.split() + response.split():\n",
    "        if word not in word2idx:\n",
    "            idx2word[len(word2idx)] = word\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "idx2word[len(word2idx)] = '<UNK>'\n",
    "word2idx['<UNK>'] = len(word2idx)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokens = []\n",
    "    for word in sentence.split():\n",
    "        if word in word2idx:\n",
    "            tokens.append(word2idx[word])\n",
    "        else:\n",
    "            tokens.append(word2idx['<UNK>'])\n",
    "    return tokens\n",
    "\n",
    "def detokenize(tokens):\n",
    "    return ' '.join([idx2word[token] for token in tokens])\n",
    "\n",
    "input_data = [tokenize(sentence) for sentence, _ in data]\n",
    "target_data = [tokenize(response) for _, response in data]\n",
    "\n",
    "def pad_sequence(seq, max_length):\n",
    "    return seq + [word2idx['<EOS>']] * (max_length - len(seq))\n",
    "\n",
    "max_length = max(len(seq) for seq in input_data + target_data)\n",
    "input_data = [pad_sequence(seq, max_length) for seq in input_data]\n",
    "target_data = [pad_sequence(seq, max_length) for seq in target_data]\n",
    "\n",
    "print(max_length)\n",
    "print(input_data[:1])\n",
    "print(target_data[:1])\n",
    "\n",
    "input_data = torch.tensor(input_data, dtype=torch.long)\n",
    "target_data = torch.tensor(target_data, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "output_dim = vocab_size\n",
    "model = ChatbotModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stat printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printstats(epoch, loss, start_time,):\n",
    "    if stoptype == \"t\":\n",
    "        time_elapsed = time.time() - start_time\n",
    "        time_epoch = time_elapsed / (epoch + 1)\n",
    "        time_left = (stop_minutes * 60) - time_elapsed\n",
    "        loss_dif = oldloss - loss.item()\n",
    "        ETA_time = time.time() + time_left\n",
    "        progress_total = 100 * (time_elapsed / (60 * stop_minutes))\n",
    "        formatted_time = datetime.datetime.fromtimestamp(ETA_time).strftime(\"%H:%M:%S\")\n",
    "        expected_epochs = time_left / time_epoch\n",
    "        \n",
    "        print(f'Epoch {epoch+1}:')\n",
    "        print(f' -  Loss:                   {loss.item():.16f}')\n",
    "        if loss_dif < 0:\n",
    "            print(f' -  Difference:            {loss_dif:.16f}')\n",
    "            print(f' -  Change:                 {\"Regressed\"}')\n",
    "        else:\n",
    "            print(f' -  Difference:             {loss_dif:.16f}')\n",
    "            print(f' -  Change:                 {\"Improved\"}')\n",
    "        print(f' -  Progress:               {progress_total:.4f}%')\n",
    "        print(f' -  Time for last epoch:    {time_epoch:.4f} seconds')\n",
    "        print(f' -  Expected epochs left:   {expected_epochs:.0f}')\n",
    "        print(f' -  Time left:              {time_left:.4f} seconds')\n",
    "        print(f' -  ETA:                    {formatted_time}')\n",
    "        print(f'')\n",
    "    \n",
    "    \n",
    "    \n",
    "    if stoptype == \"e\":\n",
    "        time_elapsed = time.time() - start_time\n",
    "        time_epoch = time_elapsed / (epoch + 1)\n",
    "        progress_total = 100 * (epoch + 1) / epochs\n",
    "        progress_left = 100 - progress_total\n",
    "        time_left = time_epoch * progress_left\n",
    "        loss_dif = oldloss - loss.item()\n",
    "        ETA_time = time.time() + time_left\n",
    "        formatted_time = datetime.datetime.fromtimestamp(ETA_time).strftime(\"%H:%M:%S\")\n",
    "        \n",
    "        \n",
    "        print(f'Epoch {epoch+1}:')\n",
    "        print(f' -  Loss:                   {loss.item():.16f}')\n",
    "        if loss_dif < 0:\n",
    "            print(f' -  Difference:            {loss_dif:.16f}')\n",
    "            print(f' -  Change:                 {\"Regressed\"}')\n",
    "        else:\n",
    "            print(f' -  Difference:             {loss_dif:.16f}')\n",
    "            print(f' -  Change:                 {\"Improved\"}')\n",
    "        print(f' -  Progress:               {progress_total:.4f}%')\n",
    "        print(f' -  Time for last epoch:    {time_epoch:.4f} seconds')\n",
    "        print(f' -  Time left:              {time_left:.4f} seconds')\n",
    "        print(f' -  ETA:                    {formatted_time}')\n",
    "        print(f'')\n",
    "    \n",
    "    \n",
    "    \n",
    "    if stoptype == \"l\":\n",
    "        time_elapsed = time.time() - start_time\n",
    "        time_epoch = time_elapsed / (epoch + 1)\n",
    "        progress_total = 100 * (loss / (first_loss - acceptable_loss))\n",
    "        progress_left = 100 - progress_total\n",
    "        loss_dif = oldloss - loss\n",
    "        time_left = progress_left * loss_dif\n",
    "        ETA_time = time.time() + time_left\n",
    "        formatted_time = datetime.datetime.fromtimestamp(ETA_time).strftime(\"%H:%M:%S\")\n",
    "        \n",
    "        \n",
    "        print(f'Epoch {epoch+1}:')\n",
    "        print(f' -  Loss:                   {loss.item():.16f}')\n",
    "        if loss_dif < 0:\n",
    "            print(f' -  Difference:            {loss_dif:.16f}')\n",
    "            print(f' -  Change:                 {\"Regressed\"}')\n",
    "        else:\n",
    "            print(f' -  Difference:             {loss_dif:.16f}')\n",
    "            print(f' -  Change:                 {\"Improved\"}')\n",
    "        print(f' -  Progress:               {progress_total:.4f}%')\n",
    "        print(f' -  Time for last epoch:    {time_epoch:.4f} seconds') \n",
    "        print(f' -  Time left:              {time_left:.4f} seconds')\n",
    "        print(f' -  ETA:                    {formatted_time}')\n",
    "        print(f'')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "data = torch.randint(0, vocab_size, (1000, 20))\n",
    "targets = torch.randint(0, vocab_size, (1000, 20))\n",
    "\n",
    "dataset = Mydataset(data, targets)\n",
    "data_loader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "start_time = time.time()\n",
    "oldloss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for input_data, target_data in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_data)\n",
    "        loss = criterion(output.view(-1, vocab_size), target_data.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch == 0:\n",
    "        first_loss = loss\n",
    "    \n",
    "    printstats(epoch, loss, start_time,)\n",
    "    if stoptype == \"l\":\n",
    "        if loss.item() < acceptable_loss:\n",
    "            print(f'Loss under {acceptable_loss}, finishing training')\n",
    "            break\n",
    "    elif stoptype == \"t\":\n",
    "        if (time.time() - start_time) / 60 >= stop_minutes:\n",
    "            print(f'Time over {stop_minutes} minutes, finishing training')\n",
    "            break\n",
    "    \n",
    "    oldloss = loss\n",
    "    if not stoptype == \"e\":\n",
    "        epochs = epochs + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_seq = torch.tensor([pad_sequence(tokenize(sentence), max_length)], dtype=torch.long)\n",
    "        output = model(input_seq)\n",
    "        output_seq = torch.argmax(output, dim=2).numpy().flatten()\n",
    "        response = []\n",
    "        for token in output_seq:\n",
    "            if idx2word[token] == '<EOS>' or idx2word[token] == '<UNK>':\n",
    "                break\n",
    "            response.append(idx2word[token])\n",
    "        response = ' '.join(response)\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = tts.init()\n",
    "voice = engine.getProperty('voices')\n",
    "engine.setProperty('voice', voice[1].id)\n",
    "def speak(speech):\n",
    "    tts.speak(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | Chat opened\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |\n",
      "    | Bot: Tipping answered climate's Miles answered yourself.You climate's yourself.You yourself.You referred excessive it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please climate's yourself.You it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please yourself.You it.Please it.Please it.Please it.Please it.Please climate's yourself.You it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please climate's yourself.You it.Please it.Please it.Please it.Please it.Please it.Please climate's yourself.You it.Please it.Please it.Please it.Please it.Please climate's yourself.You it.Please it.Please it.Please it.Please it.Please it.Please yourself.You it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please climate's yourself.You it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please climate's yourself.You it.Please it.Please it.Please it.Please it.Please it.Please yourself.You it.Please it.Please it.Please it.Please it.Please it.Please climate's yourself.You Mind it.Please it.Please it.Please it.Please climate's yourself.You it.Please it.Please it.Please it.Please it.Please climate's yourself.You it.Please it.Please it.Please climate's yourself.You it.Please it.Please it.Please climate's yourself.You Mind it.Please it.Please it.Please it.Please climate's yourself.You it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please climate's yourself.You Mind yourself.You it.Please it.Please it.Please it.Please it.Please it.Please yourself.You it.Please arcs it.Please it.Please it.Please it.Please climate's yourself.You Mind yourself.You it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please climate's yourself.You Mind it.Please it.Please it.Please it.Please arcs yourself.You it.Please arcs yourself.You Mind it.Please it.Please it.Please it.Please climate's yourself.You it.Please it.Please it.Please Mind it.Please it.Please it.Please it.Please climate's yourself.You Mind yourself.You it.Please it.Please it.Please it.Please it.Please it.Please it.Please it.Please climate's yourself.You Mind it.Please it.Please it.Please it.Please climate's yourself.You it.Please it.Please it.Please Mind yourself.You Mind it.Please it.Please climate's yourself.You Mind yourself.You it.Please it.Please it.Please it.Please yourself.You Mind it.Please it.Please climate's yourself.You Mind it.Please it.Please it.Please it.Please arcs yourself.You it.Please it.Please it.Please Mind yourself.You Mind it.Please it.Please climate's yourself.You Mind yourself.You it.Please it.Please it.Please arcs\n",
      "    |\n",
      "    | Bot: production negatively indicate land referred 22.50 smile Mind Mind smile it.Please Mind Mind it.Please Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind\n",
      "    |\n",
      "    | Bot: answered climate's smile smile smile smile smile arcs smile it.Please Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind\n",
      "    |\n",
      "    | Bot: silex nanometer cheer-section Teachers 23th climate's round-trip round-trip round-trip yourself.You yourself.You round-trip round-trip round-trip yourself.You round-trip round-trip yourself.You yourself.You round-trip Dell round-trip round-trip round-trip round-trip round-trip round-trip round-trip round-trip round-trip round-trip round-trip round-trip round-trip Louise Louise Dell Mind Mind Mind Mind Mind Mind thanks Mind Mind Mind Mind Mind Mind thanks Mind Mind Mind Mind Mind Mind thanks Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind Mind arcs yourself.You Mind Mind Mind Mind\n",
      "    |\n",
      "    | Bot: production Duanwu Tenors smile smile climate's smile smile it.Please Mind it.Please Mind Mind Mind Mind it.Please Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind Mind\n",
      "    |\n",
      "____| Quit statement used\n"
     ]
    }
   ],
   "source": [
    "def printblank():\n",
    "    print('    |')\n",
    "\n",
    "print(f'    | Chat opened')\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"quit\":\n",
    "            printblank()\n",
    "            print(f'____| Quit statement used')\n",
    "            break\n",
    "        if user_input.lower() == \"tokensplease\":\n",
    "            printblank()\n",
    "            print(f'<o> | Tokens: {tokenize(response)}')\n",
    "            continue\n",
    "        if user_input.lower() == \"wordsplease\":\n",
    "            printblank()\n",
    "            print(f'<o> | Words: {len(response.split())}')\n",
    "            continue\n",
    "        response = predict(parse(user_input))\n",
    "        printblank()\n",
    "        print(f'    | Bot: {response}')\n",
    "        #speak(response)\n",
    "    except KeyError:\n",
    "        printblank()\n",
    "        print(f'!!! | KeyError raised')\n",
    "        print(f'!!! | User: {user_input}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
